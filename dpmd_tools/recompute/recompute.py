"""Easily batch recompute large amount of structures on remote clusters.

Supports aurel and all our PCs.
Easily restart and pick up in case of fail.
All is done automatically through ssh, just need to supply structures as list
of ase."Atoms" objects.
"""

import logging
import os
import re
import signal
import sys
from importlib import import_module
from pathlib import Path
from shutil import copy, move
from typing import TYPE_CHECKING, List, Optional, Tuple

from ase.calculators.vasp import Vasp
from ase.io import write
from ase.io import read as aread
from .dispatchers import Dispatcher
from tqdm import tqdm
from file_read_backwards import FileReadBackwards

from ._remote_batch_run import RemoteBatchRun, FAILED

if TYPE_CHECKING:
    from ase.atoms import Atoms
    from dpmd_tools.recompute.json_serializer import Job

CPU_TIME = re.compile(r"\s+Total CPU time used \(sec\):\s+(\S+)\s*")
JOB_ID = re.compile(r"(?:\"|\')(\S+)(?:\"|\')")

log = logging.getLogger(__name__)


def postprocess_args(args: dict) -> dict:
    """Command line arguments post-process and expansion."""
    if len(args["user"]) == 1:
        args["user"] = args["user"] * len(args["remote"])

    if len(args["max_jobs"]) == 1:
        args["max_jobs"] = args["max_jobs"] * len(args["remote"])

    return args


# TODO implement local
class Recompute(RemoteBatchRun):
    """Class that encapsulates all the remote server analyse logic."""

    def set_constants(  # type: ignore
        self,
        *,
        scan: bool,
        data_dir: Path,
        incar_template: Optional[Path] = None,
        KP_spacing: Optional[float] = None,  # NOSONAR
    ):
        """Set constants specific for this subclass.

        Parameters
        ----------
        scan : bool
            use scan functional, has not been maintained recently might be buggy
        data_dir : Path
            directory with input data files - for vasp and job scheduler
        incar_template : Optional[Path], optional
            if supplied than INCARs will be auto generated by this class. If not
            you have to provide separate incar for each host in data dir e.g.
            `INCAR1_kohn` and inf scan is True than also `INCAR2_kohn` with the
            subsequent SCAN step. Also if this is true job scripts are auto
            generated by this class if not you again have to supply job script for
            for each server e.g. `pbs_PBE_kohn.job` or `pbs_SCAN_kohn.job` if scan
            is true.
        KP_spacing : Optional[float], optional
            if specified will be used by vasp to autogenerate Monk-Horst-Pack grid
            with defined spacing for each structure, if not supply `KPOINTS` file
            in data dir

        Warnings
        --------
        SCAN type pbs script must run both steps steps of the job. First pbs for
        which `INCAR1` will be generated and then `INACR2` the outputs POSCAR from
        the fitrst `PBE` step must be renamed by job script to `OUTCAR0`

        `KP_spacing` corresponds to `Rs` parameter in `KPOINTS` file not to
        `KSPACING` tag in incar file!
        """
        # set constants
        self.SCAN = scan
        self.DATA_DIR = data_dir
        self.KP_spacing = KP_spacing  # NOSONAR
        if incar_template:
            if not isinstance(incar_template, list):
                incar_template = [incar_template]
            self.INCAR_TEMPLATE: List[Vasp] = []
            log.info(f"loading INCAR templates for {len(incar_template)} step(s)")
            for t in tqdm(incar_template):
                templ = Vasp(xc="PBE")
                templ.read_incar(t)
                self.INCAR_TEMPLATE.append(templ)
        else:
            self.INCAR_TEMPLATE = None

        if KP_spacing:
            log.info(f"Targeting K point spacing {KP_spacing}")

        self.EXCLUDE_DOWNLOAD_FILES = (
            "*POTCAR",
            "*CHG*",
            "*DOSCAR",
            "*EIGENVAL",
            "IBZKPT",
            "*PCDAT",
            "*REPORT",
            "*WAVECAR"
        )

    # * prepare job ********************************************************************
    def prepare_calc(
        self, index: int, calc_dir: Path, server: str, atoms: "Atoms"
    ) -> Tuple[str, str]:

        log.info(f"Preparing job {index} on local side...")

        job_name = f"Label-{index}"

        # copy potential file
        copy(self.DATA_DIR / "POTCAR", calc_dir)

        # copy kpoints file
        if self.KP_spacing:
            log.info(f"setting KPOINTS spacing to {self.KP_spacing}")
            (calc_dir / "KPOINTS").write_text(self._generate_kpoints(atoms))
        else:
            log.info("copying KPOINS from inputs directory")
            copy(self.DATA_DIR / "KPOINTS", calc_dir)

        # copy first incar file
        if self.INCAR_TEMPLATE:
            log.info("INCARs and batch script will be generated automatically")
            for i, _ in enumerate(self.INCAR_TEMPLATE):
                self._get_incar(server, atoms, calc_dir, i)
        else:
            log.info("using template files for INCAR and batch script")
            copy(self.DATA_DIR / f"INCAR1_{server}", calc_dir / "INCAR.1")

        if self.SCAN:
            log.info("Running SCAN calcualtion")
            copy(self.DATA_DIR / "INCAR2", calc_dir)
            pbs = self.DATA_DIR / f"pbs_SCAN_{server}.job"
        else:
            log.info("Running PBE calcualtion")
            pbs = self.DATA_DIR / f"pbs_PBE_{server}.job"

        # write identification number and
        # copy PBS submit script to calc dir
        if self.INCAR_TEMPLATE:
            n_nodes = 1 if len(atoms) < 10 else 2
            job_script = Dispatcher.get(server)(
                n_nodes,
                job_name,
                "vasp-scan" if self.SCAN else "vasp",
                steps=len(self.INCAR_TEMPLATE),
                priority=True,
                hour_length=12,
            )
        else:
            job_script = pbs.read_text().replace("GAP_IDENT", job_name)
            if "touch done" not in job_script:
                job_script += "touch done"

        # write structure to file
        # pop is used because we do not need to keep the structure
        write(
            str(calc_dir / "POSCAR"),
            atoms,
            direct=True,
            vasp5=True,
            ignore_constraints=True,
            # TODO this messes up OUTCARs for ase, the next line will pass on
            # TODO to OUTCAR and ase will try to parse it for chem. species
            #label="File generated by python recompute script",
        )

        return job_name, job_script

    def set_job_attr(self, job: "Job"):
        job.SCAN = self.SCAN
        job.steps = len(self.INCAR_TEMPLATE)

    def _get_incar(self, server: str, atoms: "Atoms", calc_dir: Path, step: int):

        if server in ("planck", "kohn", "aurel"):
            args = {"nsim": 4, "npar": 4, "kpar": 2 if len(atoms) > 10 else 1}
        else:
            args = {"nsim": 4, "npar": 3, "kpar": 2 if len(atoms) > 10 else 1}

        if server != "aurel":
            args["lscalapack"] = False

        self.INCAR_TEMPLATE[step].set(**args)
        self.INCAR_TEMPLATE[step].set(kpts=self.KP_spacing)
        self.INCAR_TEMPLATE[step].write_incar(atoms, calc_dir)
        move(os.fspath(calc_dir / "INCAR"), os.fspath(calc_dir / f"INCAR.{step + 1}"))

    def _generate_kpoints(self, atoms: "Atoms") -> str:
        return (
            "KPOINTS spacing adjusted by recompute script\n"
            "0\n"
            "A\n"
            f"{self.KP_spacing}\n"
            "0 0 0\n"
        )

        """
        KP_STR = (
            "KPOINTS adjusted to retain same density as clathrate\n"
            "0\n"
            "M\n"
            "{} {} {}\n"
            "0 0 0\n"
        )
        POINTS = np.power(np.arange(15), 3)

        # get reciprocal volume
        vol = 1 / atoms.get_volume()
        # get num. of kpoints
        kpoints = vol / self.KP_spacing
        diff_points = POINTS - kpoints

        # get closest
        k = np.where(diff_points > 0, diff_points, np.inf).argmin()
        k = k if pow(k, 3) > kpoints else k + 1

        log.info(f"final KPOINT configuration is: {k}x{k}x{k}")
        return KP_STR.format(k, k, k)
        """

    def postprocess_job(self, job: "Job") -> Optional[float]:
        calc_dir = self.WD / job.running_dir.name

        # if job ran maximum allowed number of steps this means it has not converged
        if reached_total_steps(calc_dir, len(self.INCAR_TEMPLATE)) is True:
            log.warning(f"job {job.index} reached maximum number of steps")
            return None

        try:
            calc_time = 0
            for i, _ in enumerate(self.INCAR_TEMPLATE, 1):
                calc_time += self._get_cpu_time(calc_dir, f"OUTCAR.{i}")

            if job.SCAN:
                calc_time += self._get_cpu_time(calc_dir, "OUTCAR0")

        except (IndexError, FileNotFoundError):
            return None
        else:
            return calc_time

    @staticmethod
    def _get_cpu_time(calc_dir: Path, filename: str) -> float:

        with FileReadBackwards(calc_dir / filename) as f:
            for line in f:
                time = CPU_TIME.findall(line)
                if len(time) > 0:
                    return float(time[0])
            else:
                return 0.0


def reached_total_steps(calc_dir: Path, steps: int) -> Optional[bool]:
    """Check if the computation was stopped by reaching maximum number of steps.
    
    If job is multistep than only last step is checked and only in case it does not have
    NSW = 0
    """
    # get max number of ionic steps specified in INCAR
    incar = (calc_dir / f"INCAR.{steps}").read_text()
    nsw = int(re.findall(r"\s*NSW\s*=\s*(\d+)", incar)[0])

    # if NSW is zero skip this check
    if nsw == 0:
        log.debug(
            "skipping check if computation reached total number of steps "
            "as NSW is set to 0"
        )
        return False

    # get the number of stes from OSZICAR
    try:
        oszicar = (calc_dir / "OSZICAR").read_text()
        step = int(re.findall(r"\s*(\d+)\s+F\s*=\s*\S+\s*", oszicar)[-1])
    except (FileNotFoundError, IndexError) as e:
        log.warning(e)
        return None
    else:
        if step < nsw:
            return False
        else:
            return True


def get_incars(work_dir: Path) -> List[Path]:
    incars = list(work_dir.glob("INCAR*"))
    if len(incars) > 1 and any([i.name == "INCAR" for i in incars]):
        assert False, "Could not sort INCAR steps"
    elif len(incars) > 1:
        incars.sort(key=lambda x: int(x.name.split(".")[1]))

    return incars


def replace_failed_continue(
    atoms: List["Atoms"], work_dir: Path, failed_ids: List[int]
):

    log.info("checking jobs that where terminated due to reaching max number of steps")
    for i in failed_ids:
        path = work_dir / str(i)
        if reached_total_steps(path):
            atoms[i] = aread(work_dir / "CONTCAR", index=-1)


def recompute(args):
    args = postprocess_args(args)

    logging.basicConfig(
        level=logging.INFO,
        filename="recompute.log",
        filemode="a",
        format="[%(asctime)s] %(levelname)-7s: %(message)s",
    )

    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    # set a format which is simpler for console use
    formatter = logging.Formatter("[%(asctime)s] %(levelname)-7s: %(message)s")
    # tell the handler to use this format
    console.setFormatter(formatter)
    # add the handler to the root logger
    logging.getLogger().addHandler(console)

    logging.getLogger("paramiko").setLevel(logging.WARNING)

    loader = args["loader"]
    mod, method = loader.rsplit(".", 1)
    log.info(f"loading load data function {method} from module {mod}")
    # Â must append path since the base is elsewhere when we are running installed script
    sys.path.insert(0, os.getcwd())
    prepare_data = getattr(import_module(mod), method)

    log.info(f"Running on:     {args['remote']}")
    log.info(f"Start:          {args['start']}")
    log.info(f"Stop:           {args['end']}")
    log.info(f"Recompute:      {args['failed_recompute']}")
    log.info(f"Threaded:       {args['threaded']}")
    log.info(f"SCAN:           {args['SCAN']}")
    log.info(f"KP density:     {args['kpoint_density']}")
    log.info(f"Usernames:      {args['user']}")
    log.info(f"Max queue jobs: {args['max_jobs']}\n")

    WORK_DIR = Path.cwd()
    DATA_DIR = WORK_DIR / "data"
    DUMP_FILE = WORK_DIR / "calc_info_persistence.json"
    CVD = args["kpoint_density"]  # target kpoint spacing

    if DUMP_FILE.is_file():
        inpt = input("Dump file present, do you want to restart calculation? [y/n]: ")
        if inpt == "y":
            restart = True
        elif inpt == "n":
            restart = False
        else:
            raise ValueError(f"{inpt} answer is not supported, input y/n")
    else:
        restart = False

    SETTINGS = {}
    for r in args["remote"]:
        user = args["user"][args["remote"].index(r)]
        max_jobs = args["max_jobs"][args["remote"].index(r)]
        if r == "aurel":
            SETTINGS[r] = (
                {
                    "max_jobs": max_jobs,
                    "remote_dir": f"/gpfs/fastscratch/{user}/recompute/",
                },
            )
        else:
            SETTINGS[r] = {
                "max_jobs": max_jobs,
                "remote_dir": f"/home/{user}/Raid/recompute/",
            }

    if restart:
        r = Recompute.from_json(
            args["remote"],
            args["user"],
            args["start"],
            args["end"],
            recompute_failed=args["failed_recompute"],
            remote_settings=SETTINGS,
            dump_file=DUMP_FILE,
            threaded=args["threaded"],
        )
    else:
        r = Recompute(
            args["remote"],
            args["user"],
            args["start"],
            args["end"],
            recompute_failed=args["failed_recompute"],
            remote_settings=SETTINGS,
            work_dir=WORK_DIR,
            dump_file=DUMP_FILE,
            threaded=args["threaded"],
        )

    incars = get_incars(DATA_DIR)
    log.info(
        f"Found {len(incars)} INCARs -> "
        f"optimization will consist of {len(incars)} step(s)"
    )

    r.set_constants(
        scan=args["SCAN"],
        data_dir=DATA_DIR,
        incar_template=incars,
        KP_spacing=CVD,
    )

    log.info("loading list of ASE structures to recompute")
    atoms = prepare_data()

    if args["failed_continue"]:
        _, failed = r.get_finished_jobs(WORK_DIR, quiet=True)
        atoms = replace_failed_continue(atoms, WORK_DIR / FAILED, failed)

    log.info(f"got {len(atoms)} structures")
    r.get_job_data(atoms)
    # init_yappi()
    signal.signal(signal.SIGINT, r.handle_ctrl_c)
    r.loop()


if __name__ == "__main__":
    recompute({})
