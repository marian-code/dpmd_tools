"""Easily batch recompute large amount of structures on remote clusters.

Supports aurel and all our PCs.
Easily restart and pick up in case of fail.
All is done automatically through ssh, just need to supply structures as list
of ase."Atoms" objects.
"""

import logging
import os
import re
import signal
import sys
from importlib import import_module
from pathlib import Path
from shutil import copy
from typing import TYPE_CHECKING, Optional, Tuple

from ase.calculators.vasp import Vasp
from ase.io import write
from dpmd_tools.dispatchers import Dispatcher

from ._remote_batch_run import RemoteBatchRun

if TYPE_CHECKING:
    from ase.atoms import Atoms
    from dpmd_tools.recompute.json_serializer import Job

CPU_TIME = re.compile(r"\s+Total CPU time used \(sec\):\s+(\S+)\s*")
JOB_ID = re.compile(r"(?:\"|\')(\S+)(?:\"|\')")

log = logging.getLogger(__name__)


def postprocess_args(args: dict) -> dict:
    """Command line arguments post-process and expansion."""
    if len(args["user"]) == 1:
        args["user"] = args["user"] * len(args["remote"])

    if len(args["max_jobs"]) == 1:
        args["max_jobs"] = args["max_jobs"] * len(args["remote"])

    return args


# TODO implement local
class Recompute(RemoteBatchRun):
    """Class that encapsulates all the remote server analyse logic."""

    def set_constants(  # type: ignore
        self,
        *,
        scan: bool,
        data_dir: Path,
        incar_template: Optional[Path] = None,
        KP_spacing: Optional[float] = None,  # NOSONAR
    ):
        """Set constants specific for this subclass.

        Parameters
        ----------
        scan : bool
            use scan functional, has not been maintained recently might be buggy
        data_dir : Path
            directory with input data files - for vasp and job scheduler
        incar_template : Optional[Path], optional
            if supplied than INCARs will be auto generated by this class. If not
            you have to provide separate incar for each host in data dir e.g.
            `INCAR1_kohn` and inf scan is True than also `INCAR2_kohn` with the
            subsequent SCAN step. Also if this is true job scripts are auto
            generated by this class if not you again have to supply job script for
            for each server e.g. `pbs_PBE_kohn.job` or `pbs_SCAN_kohn.job` if scan
            is true.
        KP_spacing : Optional[float], optional
            if specified will be used by vasp to autogenerate Monk-Horst-Pack grid
            with defined spacing for each structure, if not supply `KPOINTS` file
            in data dir

        Warnings
        --------
        SCAN type pbs script must run both steps steps of the job. First pbs for
        which `INCAR1` will be generated and then `INACR2` the outputs POSCAR from
        the fitrst `PBE` step must be renamed by job script to `OUTCAR0`

        `KP_spacing` corresponds to `Rs` parameter in `KPOINTS` file not to
        `KSPACING` tag in incar file!
        """
        # set constants
        self.SCAN = scan
        self.DATA_DIR = data_dir
        self.KP_spacing = KP_spacing  # NOSONAR
        if incar_template:
            self.INCAR_TEMPLATE = Vasp(xc="PBE")
            self.INCAR_TEMPLATE.read_incar(incar_template)
        else:
            self.INCAR_TEMPLATE = None

        if KP_spacing:
            log.info(f"Targeting K point spacing {KP_spacing}")

    # * prepare job ********************************************************************
    def prepare_calc(
        self, index: int, calc_dir: Path, server: str, atoms: "Atoms"
    ) -> Tuple[str, str]:

        log.info("Preparing job on local side...")

        job_name = f"Label-{index}"

        # copy potential file
        copy(self.DATA_DIR / "POTCAR", calc_dir)

        # copy kpoints file
        if self.KP_spacing:
            log.info(f"setting KPOINTS spacing to {self.KP_spacing}")
            (calc_dir / "KPOINTS").write_text(self._generate_kpoints(atoms))
        else:
            log.info("copying KPOINS from inputs directory")
            copy(self.DATA_DIR / "KPOINTS", calc_dir)

        # copy first incar file
        if self.INCAR_TEMPLATE:
            log.info("INCAR and batch script will be generated automatically")
            self._get_incar(server, atoms, calc_dir)
        else:
            log.info("using template files for INCAR and batch script")
            copy(self.DATA_DIR / f"INCAR1_{server}", calc_dir / "INCAR")

        if self.SCAN:
            log.info("Running SCAN calcualtion")
            copy(self.DATA_DIR / "INCAR2", calc_dir)
            pbs = self.DATA_DIR / f"pbs_SCAN_{server}.job"
        else:
            log.info("Running PBE calcualtion")
            pbs = self.DATA_DIR / f"pbs_PBE_{server}.job"

        # write identification number and
        # copy PBS submit script to calc dir
        if self.INCAR_TEMPLATE:
            n_nodes = 1 if len(atoms) < 10 else 2
            job_script = Dispatcher.get(server)(
                n_nodes,
                job_name,
                "vasp-scan" if self.SCAN else "vasp",
                priority=True,
                hour_length=12,
            )
        else:
            job_script = pbs.read_text().replace("GAP_IDENT", job_name)
            if "touch done" not in job_script:
                job_script += "touch done"

        # write structure to file
        # pop is used because we do not need to keep the structure
        write(
            str(calc_dir / "POSCAR"),
            atoms,
            direct=True,
            vasp5=True,
            ignore_constraints=True,
            label="File generated by python recompute script",
        )

        return job_name, job_script

    def set_job_attr(self, job: "Job"):
        job.SCAN = self.SCAN

    def _get_incar(self, server: str, atoms: "Atoms", calc_dir: Path):

        args = {"nsim": 4, "npar": 4, "kpar": 2 if len(atoms) > 10 else 1}
        if server != "aurel":
            args["lscalapack"] = False

        self.INCAR_TEMPLATE.set(**args)
        self.INCAR_TEMPLATE.set(kpts=self.KP_spacing)
        self.INCAR_TEMPLATE.write_incar(atoms, calc_dir)

    def _generate_kpoints(self, atoms: "Atoms") -> str:
        return (
            "KPOINTS spacing adjusted by recompute script\n"
            "0\n"
            "A\n"
            f"{self.KP_spacing}\n"
            "0 0 0\n"
        )

        """
        KP_STR = (
            "KPOINTS adjusted to retain same density as clathrate\n"
            "0\n"
            "M\n"
            "{} {} {}\n"
            "0 0 0\n"
        )
        POINTS = np.power(np.arange(15), 3)

        # get reciprocal volume
        vol = 1 / atoms.get_volume()
        # get num. of kpoints
        kpoints = vol / self.KP_spacing
        diff_points = POINTS - kpoints

        # get closest
        k = np.where(diff_points > 0, diff_points, np.inf).argmin()
        k = k if pow(k, 3) > kpoints else k + 1

        log.info(f"final KPOINT configuration is: {k}x{k}x{k}")
        return KP_STR.format(k, k, k)
        """

    def postprocess_job(self, job: "Job") -> Optional[float]:
        calc_dir = self.WD / job.running_dir.name

        try:
            calc_time = self._get_cpu_time(calc_dir, "OUTCAR")

            if job.SCAN:
                calc_time += self._get_cpu_time(calc_dir, "OUTCAR0")

        except (IndexError, FileNotFoundError):
            return None
        else:
            return calc_time

    @staticmethod
    def _get_cpu_time(calc_dir, filename: str) -> float:

        output = (calc_dir / filename).read_text()
        return float(CPU_TIME.findall(output)[0])


def recompute(args):
    args = postprocess_args(args)

    logging.basicConfig(
        level=logging.INFO,
        format="[%(asctime)s] %(levelname)-7s: %(message)s",
    )
    logging.getLogger("paramiko").setLevel(logging.WARNING)

    loader = args["loader"]
    mod, method = loader.rsplit(".", 1)
    log.info(f"loading load data function {method} from module {mod}")
    #Â must append path since the base is elsewhere when we are running installed script
    sys.path.insert(0, os.getcwd())
    prepare_data = getattr(import_module(mod), method)

    log.info(f"Running on:     {args['remote']}")
    log.info(f"Start:          {args['start']}")
    log.info(f"Stop:           {args['end']}")
    log.info(f"Recompute:      {args['failed_recompute']}")
    log.info(f"Threaded:       {args['threaded']}")
    log.info(f"SCAN:           {args['SCAN']}")
    log.info(f"Usernames:      {args['user']}")
    log.info(f"Max queue jobs: {args['max_jobs']}\n")

    WORK_DIR = Path.cwd()
    DATA_DIR = WORK_DIR / "data"
    DUMP_FILE = WORK_DIR / "calc_info_persistence.json"
    CVD = 20  # target kpoint spacing

    if DUMP_FILE.is_file():
        inpt = input("Dump file present, do you want to restart calculation? [y/n]: ")
        if inpt == "y":
            restart = True
        elif inpt == "n":
            restart = False
        else:
            raise ValueError(f"{inpt} answer is not supported, input y/n")
    else:
        restart = False

    SETTINGS = {}
    for r in args["remote"]:
        user = args["user"][args["remote"].index(r)]
        max_jobs = args["max_jobs"][args["remote"].index(r)]
        if r == "aurel":
            SETTINGS[r] = (
                {
                    "max_jobs": max_jobs,
                    "remote_dir": f"/gpfs/fastscratch/{user}/recompute/",
                },
            )
        else:
            SETTINGS[r] = {
                "max_jobs": max_jobs,
                "remote_dir": f"/home/{user}/Raid/recompute/",
            }

    if restart:
        r = Recompute.from_json(
            args["remote"],
            args["user"],
            args["start"],
            args["end"],
            recompute_failed=args["failed_recompute"],
            remote_settings=SETTINGS,
            dump_file=DUMP_FILE,
            threaded=args["threaded"],
        )
    else:
        r = Recompute(
            args["remote"],
            args["user"],
            args["start"],
            args["end"],
            recompute_failed=args["failed_recompute"],
            remote_settings=SETTINGS,
            work_dir=WORK_DIR,
            dump_file=DUMP_FILE,
            threaded=args["threaded"],
        )

    r.set_constants(
        scan=args["SCAN"],
        data_dir=DATA_DIR,
        incar_template=DATA_DIR / "INCAR",
        KP_spacing=CVD,
    )

    log.info("loading list of ASE structures to recompute")
    atoms = prepare_data()
    log.info(f"got {len(atoms)} structures")
    r.get_job_data(atoms)
    # init_yappi()
    signal.signal(signal.SIGINT, r.handle_ctrl_c)
    r.loop()


if __name__ == "__main__":
    recompute({})
